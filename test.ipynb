{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Generate random pandas dataframe of 4 columns and 100 rows for regression based on equation y = 2x1 + 3x2 + 4x3 + 5x4 + 23.2 + noise\n",
    "x = np.random.rand(1000, 4)\n",
    "y = 2*x[:, 0] + 3*x[:, 1] + 4*x[:, 2] + 5*x[:, 3] + 23.2 + np.random.randn(1000)*0.01\n",
    "# store x and y as pandas dataframe wiht columns as x1, x2, x3, x4 and y\n",
    "df = pd.DataFrame(data=np.concatenate((x, y.reshape(-1, 1)), axis=1), columns=['x1', 'x2', 'x3', 'x4', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.169568</td>\n",
       "      <td>0.143362</td>\n",
       "      <td>0.751399</td>\n",
       "      <td>0.203586</td>\n",
       "      <td>27.989815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.825794</td>\n",
       "      <td>0.966376</td>\n",
       "      <td>0.834363</td>\n",
       "      <td>0.291296</td>\n",
       "      <td>32.543033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.144124</td>\n",
       "      <td>0.960403</td>\n",
       "      <td>0.453643</td>\n",
       "      <td>0.549063</td>\n",
       "      <td>30.935379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.672083</td>\n",
       "      <td>0.784020</td>\n",
       "      <td>0.194028</td>\n",
       "      <td>0.257796</td>\n",
       "      <td>28.963049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.781745</td>\n",
       "      <td>0.196509</td>\n",
       "      <td>0.713197</td>\n",
       "      <td>0.964692</td>\n",
       "      <td>33.048228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.818568</td>\n",
       "      <td>0.926017</td>\n",
       "      <td>0.119169</td>\n",
       "      <td>0.435418</td>\n",
       "      <td>30.274995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.098247</td>\n",
       "      <td>0.323721</td>\n",
       "      <td>0.158708</td>\n",
       "      <td>0.835027</td>\n",
       "      <td>29.182915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.645390</td>\n",
       "      <td>0.957386</td>\n",
       "      <td>0.405065</td>\n",
       "      <td>0.129360</td>\n",
       "      <td>29.617366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.217419</td>\n",
       "      <td>0.953476</td>\n",
       "      <td>0.278034</td>\n",
       "      <td>0.854730</td>\n",
       "      <td>31.876127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.397676</td>\n",
       "      <td>0.785992</td>\n",
       "      <td>0.581606</td>\n",
       "      <td>0.513345</td>\n",
       "      <td>31.245357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1        x2        x3        x4          y\n",
       "0    0.169568  0.143362  0.751399  0.203586  27.989815\n",
       "1    0.825794  0.966376  0.834363  0.291296  32.543033\n",
       "2    0.144124  0.960403  0.453643  0.549063  30.935379\n",
       "3    0.672083  0.784020  0.194028  0.257796  28.963049\n",
       "4    0.781745  0.196509  0.713197  0.964692  33.048228\n",
       "..        ...       ...       ...       ...        ...\n",
       "995  0.818568  0.926017  0.119169  0.435418  30.274995\n",
       "996  0.098247  0.323721  0.158708  0.835027  29.182915\n",
       "997  0.645390  0.957386  0.405065  0.129360  29.617366\n",
       "998  0.217419  0.953476  0.278034  0.854730  31.876127\n",
       "999  0.397676  0.785992  0.581606  0.513345  31.245357\n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_examples': 1000, 'training_steps': 100, 'display_step': 1, 'learning_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_ml.regression.linear_regression import LinearRegression\n",
    "lr = LinearRegression()\n",
    "\n",
    "_params = {\n",
    "    \"n_examples\": len(df),\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"training_steps\": 100,\n",
    "    \"display_step\": 1,\n",
    "}\n",
    "\n",
    "lr.set_params(_params)\n",
    "\n",
    "print(lr.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the x1,x2,x3,x4 into np.ndarray from dataframe\n",
    "# x = df[['x1']].to_numpy()\n",
    "x = df[['x1', 'x2', 'x3', 'x4']].to_numpy()\n",
    "y = df['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 00: 926.962564\n",
      "Loss at step 01: 854.186165\n",
      "Loss at step 02: 787.279766\n",
      "Loss at step 03: 725.769682\n",
      "Loss at step 04: 669.220453\n",
      "Loss at step 05: 617.231759\n",
      "Loss at step 06: 569.435585\n",
      "Loss at step 07: 525.493613\n",
      "Loss at step 08: 485.094827\n",
      "Loss at step 09: 447.953307\n",
      "Loss at step 10: 413.806208\n",
      "Loss at step 11: 382.411896\n",
      "Loss at step 12: 353.548232\n",
      "Loss at step 13: 327.011009\n",
      "Loss at step 14: 302.612493\n",
      "Loss at step 15: 280.180104\n",
      "Loss at step 16: 259.555186\n",
      "Loss at step 17: 240.591887\n",
      "Loss at step 18: 223.156125\n",
      "Loss at step 19: 207.124636\n",
      "Loss at step 20: 192.384106\n",
      "Loss at step 21: 178.830363\n",
      "Loss at step 22: 166.367642\n",
      "Loss at step 23: 154.907906\n",
      "Loss at step 24: 144.370222\n",
      "Loss at step 25: 134.680188\n",
      "Loss at step 26: 125.769406\n",
      "Loss at step 27: 117.574997\n",
      "Loss at step 28: 110.039159\n",
      "Loss at step 29: 103.108749\n",
      "Loss at step 30: 96.734918\n",
      "Loss at step 31: 90.872756\n",
      "Loss at step 32: 85.480977\n",
      "Loss at step 33: 80.521628\n",
      "Loss at step 34: 75.959818\n",
      "Loss at step 35: 71.763470\n",
      "Loss at step 36: 67.903099\n",
      "Loss at step 37: 64.351596\n",
      "Loss at step 38: 61.084042\n",
      "Loss at step 39: 58.077527\n",
      "Loss at step 40: 55.310991\n",
      "Loss at step 41: 52.765073\n",
      "Loss at step 42: 50.421974\n",
      "Loss at step 43: 48.265333\n",
      "Loss at step 44: 46.280106\n",
      "Loss at step 45: 44.452466\n",
      "Loss at step 46: 42.769702\n",
      "Loss at step 47: 41.220125\n",
      "Loss at step 48: 39.792994\n",
      "Loss at step 49: 38.478431\n",
      "Loss at step 50: 37.267357\n",
      "Loss at step 51: 36.151426\n",
      "Loss at step 52: 35.122964\n",
      "Loss at step 53: 34.174917\n",
      "Loss at step 54: 33.300801\n",
      "Loss at step 55: 32.494653\n",
      "Loss at step 56: 31.750995\n",
      "Loss at step 57: 31.064786\n",
      "Loss at step 58: 30.431396\n",
      "Loss at step 59: 29.846567\n",
      "Loss at step 60: 29.306385\n",
      "Loss at step 61: 28.807250\n",
      "Loss at step 62: 28.345856\n",
      "Loss at step 63: 27.919161\n",
      "Loss at step 64: 27.524369\n",
      "Loss at step 65: 27.158910\n",
      "Loss at step 66: 26.820422\n",
      "Loss at step 67: 26.506731\n",
      "Loss at step 68: 26.215843\n",
      "Loss at step 69: 25.945920\n",
      "Loss at step 70: 25.695276\n",
      "Loss at step 71: 25.462359\n",
      "Loss at step 72: 25.245742\n",
      "Loss at step 73: 25.044115\n",
      "Loss at step 74: 24.856273\n",
      "Loss at step 75: 24.681107\n",
      "Loss at step 76: 24.517598\n",
      "Loss at step 77: 24.364811\n",
      "Loss at step 78: 24.221884\n",
      "Loss at step 79: 24.088026\n",
      "Loss at step 80: 23.962509\n",
      "Loss at step 81: 23.844665\n",
      "Loss at step 82: 23.733878\n",
      "Loss at step 83: 23.629584\n",
      "Loss at step 84: 23.531262\n",
      "Loss at step 85: 23.438435\n",
      "Loss at step 86: 23.350663\n",
      "Loss at step 87: 23.267544\n",
      "Loss at step 88: 23.188705\n",
      "Loss at step 89: 23.113806\n",
      "Loss at step 90: 23.042532\n",
      "Loss at step 91: 22.974596\n",
      "Loss at step 92: 22.909732\n",
      "Loss at step 93: 22.847696\n",
      "Loss at step 94: 22.788265\n",
      "Loss at step 95: 22.731232\n",
      "Loss at step 96: 22.676408\n",
      "Loss at step 97: 22.623618\n",
      "Loss at step 98: 22.572704\n",
      "Loss at step 99: 22.523517\n"
     ]
    }
   ],
   "source": [
    "lr.fit(x, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error :\n",
      "\t 5.221607404256912\n",
      "Predictions :\n",
      "\t [24.18415153 34.76093872 30.07292884 27.82969632 33.95596194]\n",
      "Actual :\n",
      "\t [27.98981501 32.54303345 30.9353789  28.96304923 33.04822813]\n",
      "Model coefficients:\n",
      "\tW = [5.87397123 6.59656796 7.11668241 8.01096346]\n",
      "\tB = 15.264026186754252\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Squared Error :\\n\\t\", lr.evaluate())\n",
    "print(\"Predictions :\\n\\t\", lr.predict(x[:5]))\n",
    "print(\"Actual :\\n\\t\", y[:5])\n",
    "\n",
    "coeffs = lr.get_coeffs()\n",
    "print(\"Model coefficients:\\n\\tW = {}\\n\\tB = {}\".format(coeffs[\"weight\"], coeffs[\"bias\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Therefore, the predicted model equation is:\n",
      "\ty = 5.8740 * x[:, 0] + 6.596568 * x[:, 1] + 7.1167 * x[:, 2] + 8.0110 * x[:, 3] + 15.2640\n",
      "The actual model equation is:\n",
      "\ty = 2 * x[:, 0] + 3 * x[:, 1] + 4 * x[:, 2] + 5 * x[:, 3] + 23.2 + NOISE\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTherefore, the predicted model equation is:\\n\\ty = {:.4f} * x[:, 0] + {:4f} * x[:, 1] + {:.4f} * x[:, 2] + {:.4f} * x[:, 3] + {:.4f}\"\n",
    "      .format(coeffs[\"weight\"][0], coeffs[\"weight\"][1], coeffs[\"weight\"][2], coeffs[\"weight\"][3], coeffs[\"bias\"]))\n",
    "print(\"The actual model equation is:\\n\\ty = 2 * x[:, 0] + 3 * x[:, 1] + 4 * x[:, 2] + 5 * x[:, 3] + 23.2 + NOISE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a linear regression model using sklearn\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "\n",
    "lr2 = LR()\n",
    "lr2.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.99891337, 2.99973158, 3.99949592, 5.00054725])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.199997114717892"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'positive': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyalgo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
